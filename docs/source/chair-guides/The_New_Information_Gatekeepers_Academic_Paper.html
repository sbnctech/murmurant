<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The New Information Gatekeepers: AI Chatbots, Structured Data, and the Future of Political Communication</title>
    <style>
        :root {
            --primary-color: #1a365d;
            --secondary-color: #2c5282;
            --accent-color: #3182ce;
            --text-color: #1a202c;
            --light-bg: #f7fafc;
            --border-color: #e2e8f0;
        }
        
        * {
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Georgia', 'Times New Roman', serif;
            line-height: 1.8;
            color: var(--text-color);
            max-width: 850px;
            margin: 0 auto;
            padding: 40px 20px;
            background: #fff;
        }
        
        /* Title Page */
        .title-page {
            text-align: center;
            padding: 60px 20px;
            margin-bottom: 60px;
            border-bottom: 2px solid var(--border-color);
        }
        
        .title-page h1 {
            font-size: 2.2em;
            color: var(--primary-color);
            margin-bottom: 20px;
            line-height: 1.3;
        }
        
        .title-page .subtitle {
            font-size: 1.3em;
            color: var(--secondary-color);
            font-style: italic;
            margin-bottom: 40px;
        }
        
        .title-page .author {
            font-size: 1.1em;
            margin-bottom: 10px;
        }
        
        .title-page .affiliation {
            font-size: 1em;
            color: #666;
            margin-bottom: 30px;
        }
        
        .title-page .date {
            font-size: 1em;
            color: #666;
        }
        
        .title-page .disclosure {
            font-size: 0.9em;
            color: #666;
            font-style: italic;
            margin-top: 40px;
            padding: 20px;
            background: var(--light-bg);
            border-radius: 8px;
            text-align: left;
        }
        
        /* Abstract */
        .abstract {
            background: var(--light-bg);
            padding: 30px;
            border-left: 4px solid var(--accent-color);
            margin-bottom: 40px;
        }
        
        .abstract h2 {
            font-size: 1.2em;
            color: var(--primary-color);
            margin-top: 0;
            margin-bottom: 15px;
        }
        
        .abstract p {
            margin: 0;
            font-size: 0.95em;
        }
        
        .keywords {
            margin-top: 20px;
            font-size: 0.9em;
        }
        
        .keywords strong {
            color: var(--primary-color);
        }
        
        /* Section Headers */
        h2 {
            font-size: 1.5em;
            color: var(--primary-color);
            margin-top: 50px;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--border-color);
        }
        
        h3 {
            font-size: 1.2em;
            color: var(--secondary-color);
            margin-top: 35px;
            margin-bottom: 15px;
        }
        
        h4 {
            font-size: 1.05em;
            color: var(--text-color);
            margin-top: 25px;
            margin-bottom: 12px;
        }
        
        /* Paragraphs */
        p {
            margin-bottom: 18px;
            text-align: justify;
        }
        
        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 25px 0;
            font-size: 0.9em;
        }
        
        th {
            background: var(--primary-color);
            color: white;
            padding: 12px 15px;
            text-align: left;
            font-weight: 600;
        }
        
        td {
            padding: 12px 15px;
            border-bottom: 1px solid var(--border-color);
        }
        
        tr:nth-child(even) {
            background: var(--light-bg);
        }
        
        /* Blockquotes */
        blockquote {
            margin: 25px 0;
            padding: 20px 25px;
            background: var(--light-bg);
            border-left: 4px solid var(--accent-color);
            font-style: italic;
        }
        
        blockquote p {
            margin: 0;
        }
        
        blockquote cite {
            display: block;
            margin-top: 10px;
            font-style: normal;
            font-size: 0.9em;
            color: #666;
        }
        
        /* Callout Boxes */
        .callout {
            background: #fff3cd;
            border: 1px solid #ffc107;
            border-radius: 8px;
            padding: 20px;
            margin: 25px 0;
        }
        
        .callout-important {
            background: #f8d7da;
            border-color: #dc3545;
        }
        
        .callout-info {
            background: #cce5ff;
            border-color: #007bff;
        }
        
        .callout h4 {
            margin-top: 0;
            color: var(--primary-color);
        }
        
        /* Footnotes */
        .footnote-ref {
            font-size: 0.75em;
            vertical-align: super;
            color: var(--accent-color);
            text-decoration: none;
            font-weight: 600;
        }
        
        .footnote-ref:hover {
            text-decoration: underline;
        }
        
        .footnotes {
            margin-top: 60px;
            padding-top: 30px;
            border-top: 2px solid var(--border-color);
            font-size: 0.85em;
        }
        
        .footnotes h2 {
            font-size: 1.3em;
        }
        
        .footnote {
            margin-bottom: 12px;
            padding-left: 25px;
            position: relative;
        }
        
        .footnote-number {
            position: absolute;
            left: 0;
            color: var(--accent-color);
            font-weight: 600;
        }
        
        .footnote a {
            color: var(--accent-color);
            word-break: break-all;
        }
        
        /* Lists */
        ul, ol {
            margin: 20px 0;
            padding-left: 30px;
        }
        
        li {
            margin-bottom: 10px;
        }
        
        /* Statistics highlight */
        .stat-highlight {
            font-weight: 700;
            color: var(--primary-color);
        }
        
        /* Figure captions */
        .figure {
            margin: 30px 0;
            text-align: center;
        }
        
        .figure-caption {
            font-size: 0.9em;
            color: #666;
            font-style: italic;
            margin-top: 10px;
        }
        
        /* Print styles */
        @media print {
            body {
                font-size: 11pt;
                max-width: 100%;
            }
            
            h2 {
                page-break-after: avoid;
            }
            
            table, blockquote {
                page-break-inside: avoid;
            }
        }
        
        /* Table of Contents */
        .toc {
            background: var(--light-bg);
            padding: 30px;
            margin-bottom: 40px;
            border-radius: 8px;
        }
        
        .toc h2 {
            margin-top: 0;
            border-bottom: none;
            padding-bottom: 0;
        }
        
        .toc ul {
            list-style: none;
            padding-left: 0;
        }
        
        .toc li {
            margin-bottom: 8px;
        }
        
        .toc a {
            color: var(--secondary-color);
            text-decoration: none;
        }
        
        .toc a:hover {
            text-decoration: underline;
        }
        
        .toc .toc-h3 {
            padding-left: 20px;
            font-size: 0.95em;
        }
    </style>
</head>
<body>

<!-- Title Page -->
<div class="title-page">
    <h1>The New Information Gatekeepers: AI Chatbots, Structured Data, and the Future of Political Communication</h1>
    <div class="subtitle">How Evidence-Based Messaging and JSON-LD Schema Markup Will Shape Democratic Discourse in the Age of Artificial Intelligence</div>
    
    <div class="author">Edward A. Forman</div>
    <div class="affiliation">Independent Researcher</div>
    <div class="date">December 2025</div>
    
    <div class="disclosure">
        <strong>Disclosure:</strong> The author is the founder of KyanosTech, a political technology company developing AI optimization tools for progressive campaigns. This paper represents independent academic analysis informed by that perspective. The author believes transparency about potential conflicts of interest strengthens rather than undermines scholarly inquiry. All factual claims are independently verifiable through cited sources.
    </div>
</div>

<!-- Abstract -->
<div class="abstract">
    <h2>Abstract</h2>
    <p>
        By 2028, an estimated 800 million weekly users will consult AI chatbots like ChatGPT, Claude, and Google's AI Overviews for information—including information about political candidates and elections. This paper examines how AI chatbots are transforming from novelty to dominant information medium, fundamentally altering how voters encounter political content. Drawing on research demonstrating that AI chatbots can shift political views with statistical significance (β=0.98, p<0.01) even when users recognize bias, alongside evidence of 50%+ error rates on election information, the paper argues that the AI information ecosystem represents both unprecedented opportunity and risk for democratic discourse.
    </p>
    <p style="margin-top: 15px;">
        The paper advances three central arguments. First, the structural requirements of AI chatbots—verification, citation, evidence-based reasoning—align more naturally with Democratic communication patterns than Republican ones, creating asymmetric advantages that cannot be addressed through bias accusations alone. Second, JSON-LD schema markup provides independent value for political campaigns regardless of how AI companies navigate political pressures, analogous to OpenAI's recently launched "express lane" for e-commerce merchants. Third, the philosophical impossibility of political neutrality, combined with unprecedented financial and political pressure on AI companies, necessitates independent university-based evaluation to maintain democratic information integrity. The paper concludes with recommendations for schema standard development, academic research priorities, and strategic considerations for campaigns seeking to optimize their presence in AI-mediated information environments.
    </p>
    
    <div class="keywords">
        <strong>Keywords:</strong> artificial intelligence, political communication, chatbots, schema markup, JSON-LD, misinformation, democratic discourse, structured data, media evolution, political bias
    </div>
</div>

<!-- Table of Contents -->
<div class="toc">
    <h2>Contents</h2>
    <ul>
        <li><a href="#introduction">I. Introduction: The Podcast Mistake Redux</a></li>
        <li><a href="#medium-evolution">II. From Search to Synthesis: The New Information Architecture</a></li>
        <li class="toc-h3"><a href="#chatbot-growth">A. The Explosive Growth of AI Chatbots</a></li>
        <li class="toc-h3"><a href="#prose-vs-lists">B. From Ranked Lists to Authoritative Prose</a></li>
        <li class="toc-h3"><a href="#websites-repositories">C. Websites as AI Information Repositories</a></li>
        <li><a href="#influence">III. The Power to Persuade: AI's Documented Political Influence</a></li>
        <li class="toc-h3"><a href="#fisher-study">A. The Fisher Study: Bias That Works Even When Recognized</a></li>
        <li class="toc-h3"><a href="#accuracy-crisis">B. The Accuracy Crisis in Political Information</a></li>
        <li><a href="#structural-alignment">IV. Structural Alignment: Why AI Requirements Favor Evidence-Based Communication</a></li>
        <li class="toc-h3"><a href="#rlhf">A. How AI Chatbots Are Trained</a></li>
        <li class="toc-h3"><a href="#evidence-asymmetry">B. The Evidence Asymmetry Between Parties</a></li>
        <li class="toc-h3"><a href="#truth-bias">C. The Truth-Bias Paradox</a></li>
        <li><a href="#pressure">V. Under Pressure: Financial and Political Forces on AI Companies</a></li>
        <li class="toc-h3"><a href="#federal-contracts">A. Federal Contract Dependency</a></li>
        <li class="toc-h3"><a href="#investor-alignment">B. Investor Political Alignment</a></li>
        <li class="toc-h3"><a href="#woke-ai">C. The "Woke AI" Pressure Campaign</a></li>
        <li><a href="#neutrality">VI. The Impossibility of Neutrality</a></li>
        <li><a href="#schema">VII. JSON-LD and the Political Information Commons</a></li>
        <li class="toc-h3"><a href="#independent-value">A. Independent Value Regardless of AI Company Policies</a></li>
        <li class="toc-h3"><a href="#current-standards">B. Current Schema Standards and the Political Content Gap</a></li>
        <li class="toc-h3"><a href="#ecommerce-precedent">C. The E-Commerce Precedent</a></li>
        <li><a href="#university">VIII. The Case for University-Based Evaluation</a></li>
        <li><a href="#recommendations">IX. Recommendations and Conclusions</a></li>
        <li><a href="#footnotes">Notes</a></li>
    </ul>
</div>

<!-- Section I: Introduction -->
<h2 id="introduction">I. Introduction: The Podcast Mistake Redux</h2>

<p>
In the decade between 2014 and 2024, podcasting emerged as a transformative force in American political communication. While progressives invested in traditional media and digital advertising, conservatives built an expansive podcast infrastructure that reached millions of Americans weekly. By the time progressive organizations recognized the medium's significance, conservative voices dominated the landscape—from Joe Rogan's 11 million listeners per episode to Ben Shapiro's Daily Wire empire.<a href="#fn1" class="footnote-ref">1</a> The "podcast mistake," as some progressive strategists now call it, represented a failure not of ideology but of infrastructure: the medium itself was politically neutral, but the failure to build competing infrastructure ceded enormous influence to political opponents.
</p>

<p>
This paper argues that American politics stands at a similar inflection point with artificial intelligence chatbots. The numbers are stark: ChatGPT now serves <span class="stat-highlight">800 million weekly active users</span>, up from 50 million in January 2023—a sixteen-fold increase in under three years.<a href="#fn2" class="footnote-ref">2</a> Google's AI Overviews now appear in more than <span class="stat-highlight">50% of all Google searches</span>, meaning over a billion users monthly encounter AI-synthesized responses, often without realizing it.<a href="#fn3" class="footnote-ref">3</a> OpenAI projects one billion users by the end of 2025.<a href="#fn4" class="footnote-ref">4</a> The chatbot market, valued at $7.76 billion in 2024, is projected to reach $27 billion by 2030.<a href="#fn5" class="footnote-ref">5</a>
</p>

<p>
Unlike podcasting, however, AI chatbots present a more complex challenge. The medium itself is not neutral in the way radio frequencies or RSS feeds are neutral. AI systems are trained through processes that embed values—accuracy, helpfulness, harmlessness—and these training objectives interact with political content in ways that have measurable effects on how different political positions are represented. Research from Stanford demonstrates that users perceive AI chatbots as exhibiting left-leaning bias across 24 different large language models.<a href="#fn6" class="footnote-ref">6</a> Yet research from MIT suggests something more profound: when AI systems are trained purely on factual statements, they still produce outputs perceived as left-leaning, raising uncomfortable questions about the relationship between evidence-based reasoning and political positioning.<a href="#fn7" class="footnote-ref">7</a>
</p>

<p>
This paper examines the emerging AI chatbot ecosystem through a lens concerned with progressive political outcomes, but its analysis aims at rigor rather than advocacy. The central questions are empirical: How do AI chatbots influence political attitudes? What structural factors shape how different political positions are represented? What role can structured data play in ensuring accurate representation? And what independent evaluation mechanisms are needed to maintain democratic information integrity?
</p>

<p>
The paper proceeds in nine sections. Section II documents the transformation of information architecture from search-based discovery to AI-synthesized responses. Section III examines the documented evidence of AI political influence, including research showing that biased AI chatbots can shift political views even when users recognize the bias. Section IV analyzes the structural alignment between AI training requirements and evidence-based communication patterns, arguing that Democratic communication advantages in research citation and fact-check accuracy create systematic benefits in AI-mediated environments. Section V examines the unprecedented financial and political pressures facing AI companies, from federal contracts to investor alignment to the Trump administration's "woke AI" pressure campaign. Section VI addresses the philosophical impossibility of political neutrality and its implications. Section VII presents the case for JSON-LD structured data as infrastructure that provides value regardless of AI company policies. Section VIII argues for university-based independent evaluation. Section IX offers recommendations and conclusions.
</p>

<blockquote>
    <p>"Infrastructure before persuasion. Otherwise, we're reliving the podcast mistake."</p>
</blockquote>

<!-- Section II: Medium Evolution -->
<h2 id="medium-evolution">II. From Search to Synthesis: The New Information Architecture</h2>

<p>
The transformation from traditional search engines to AI chatbots represents more than a technological upgrade—it constitutes a fundamental restructuring of how humans encounter and process information. Understanding this shift is essential to grasping both the opportunities and risks AI chatbots present for democratic discourse.
</p>

<h3 id="chatbot-growth">A. The Explosive Growth of AI Chatbots</h3>

<p>
The adoption curve for AI chatbots exceeds that of virtually any previous information technology. ChatGPT's user growth tells a striking story: from launch in November 2022, the platform reached 100 million users by January 2023—the fastest application in history to that milestone.<a href="#fn8" class="footnote-ref">8</a> By February 2025, weekly active users had reached 400 million; by March 2025, that figure had doubled to 800 million.<a href="#fn9" class="footnote-ref">9</a> ChatGPT is now the sixth most visited website globally, with 5.8 billion monthly visits.<a href="#fn10" class="footnote-ref">10</a>
</p>

<table>
    <thead>
        <tr>
            <th>Platform</th>
            <th>Users/Reach</th>
            <th>Growth Trajectory</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>ChatGPT</td>
            <td>800M weekly active users</td>
            <td>50M → 800M in 26 months (1,500% growth)</td>
        </tr>
        <tr>
            <td>Google AI Overviews</td>
            <td>50%+ of all searches</td>
            <td>25% (Aug 2024) → 50%+ (2025)</td>
        </tr>
        <tr>
            <td>Claude (Anthropic)</td>
            <td>Not publicly disclosed</td>
            <td>Significant enterprise adoption</td>
        </tr>
        <tr>
            <td>Perplexity AI</td>
            <td>15M+ monthly users</td>
            <td>Fastest-growing AI search engine</td>
        </tr>
    </tbody>
</table>

<p>
These figures, impressive as they are, understate AI's penetration into information-seeking behavior. Google's AI Overviews—AI-generated summaries that appear above traditional search results—now trigger on <span class="stat-highlight">13.14% of all search queries</span>, up from 6.49% in January 2025, representing 72% growth in just two months.<a href="#fn11" class="footnote-ref">11</a> Because this represents a modification of existing Google search behavior rather than adoption of a new platform, users may not realize they are receiving AI-mediated responses. The majority of users encountering AI Overviews have "informational intent"—<span class="stat-highlight">99.2%</span> of queries triggering AI Overviews are informational rather than navigational or transactional.<a href="#fn12" class="footnote-ref">12</a> This is precisely the type of query voters use when researching candidates and issues.
</p>

<p>
Demographic patterns suggest AI chatbot usage will intensify among politically engaged populations. Pew Research found that <span class="stat-highlight">43% of adults ages 18-29 have used ChatGPT</span>, compared to 23% of all adults.<a href="#fn13" class="footnote-ref">13</a> Among Fortune 500 companies, 92% now use ChatGPT for business purposes.<a href="#fn14" class="footnote-ref">14</a> The trajectory suggests that by the 2028 election cycle, AI chatbots will be as ubiquitous as social media was by 2016 or podcasts were by 2024.
</p>

<h3 id="prose-vs-lists">B. From Ranked Lists to Authoritative Prose</h3>

<p>
The shift from search engines to AI chatbots represents more than a change in interface—it fundamentally alters the epistemological relationship between users and information.
</p>

<p>
<strong>Traditional search engines</strong> operated through a model of curation: users submitted queries and received ranked lists of links to external sources. The user retained agency over information synthesis—evaluating multiple sources, comparing claims, and forming conclusions. The search engine's role was selection and ranking, not interpretation. When a user searched for "healthcare policy positions," Google returned links to candidate websites, news articles, fact-checks, and advocacy organizations. The user navigated among these sources, bringing their own judgment to bear on competing claims.
</p>

<p>
<strong>AI chatbots</strong> operate through a model of synthesis: users submit queries and receive prose responses that integrate information from multiple sources into coherent narratives. The chatbot does not merely curate—it interprets, summarizes, and presents conclusions. When a user asks ChatGPT about healthcare policy positions, they receive a synthesized response that has already evaluated sources, weighted competing claims, and reached conclusions about what information is most relevant and accurate.
</p>

<p>
This shift has profound implications for political information. As one analysis noted, AI-generated text "looks authoritative and cites sources," creating an "air of correctness that might not actually be there."<a href="#fn15" class="footnote-ref">15</a> MIT Technology Review reported that "a lot of people don't check citations" when AI responses appear authoritative.<a href="#fn16" class="footnote-ref">16</a> The conversational format creates false intimacy and trust—users interact with AI chatbots as they would with knowledgeable friends rather than as they would with search results.
</p>

<p>
Perhaps most significantly, unlike search results, chatbot responses do not present alternative perspectives side-by-side. When a traditional search returned links to both a candidate's website and their opponent's attack ads, the user encountered competing narratives simultaneously. When a chatbot synthesizes information about that candidate, the response presents a singular narrative—one that reflects the chatbot's training, the available data, and the system's internal weighting of sources.
</p>

<h3 id="websites-repositories">C. Websites as AI Information Repositories</h3>

<p>
The rise of AI chatbots is transforming the fundamental purpose of websites. Traditionally, websites served as <em>destinations</em>—places users visited to consume content directly. Design priorities emphasized human factors: visual appeal, navigation, user experience, conversion optimization. Success was measured by page views, time on site, and user engagement.
</p>

<p>
In the AI era, websites increasingly serve as <em>repositories</em>—sources from which AI systems extract information to synthesize responses elsewhere. The user may never visit the website directly; instead, they encounter the website's content through AI-mediated summaries. This shift has measurable effects: research shows a <span class="stat-highlight">34.5% drop in click-through rates</span> for top-ranking pages when AI Overviews appear.<a href="#fn17" class="footnote-ref">17</a> Sessions ending after viewing an AI Overview reach 26%, compared to 16% for standard search results.<a href="#fn18" class="footnote-ref">18</a> Only 1% of visits to pages with AI summaries result in clicks on links within the summary.<a href="#fn19" class="footnote-ref">19</a>
</p>

<p>
For political campaigns, this transformation has immediate implications. Campaign websites have traditionally been designed for human voters—emphasizing visual design, emotional appeal, and conversion to donations or volunteer signups. In the AI era, campaign websites must be designed for machine comprehension—ensuring that AI systems accurately understand and represent candidate positions, policy details, and supporting evidence.
</p>

<p>
The campaigns that fail to adapt will find their candidates misrepresented by AI systems that infer positions from incomplete or outdated training data. The campaigns that adapt—through structured data markup, evidence-linked claims, and machine-readable policy positions—will ensure accurate representation regardless of how AI companies navigate political pressures.
</p>

<!-- Section III: Influence -->
<h2 id="influence">III. The Power to Persuade: AI's Documented Political Influence</h2>

<p>
The question of whether AI chatbots can influence political attitudes is no longer theoretical. A growing body of rigorous research demonstrates that AI systems can shift political views with statistical significance—and, more disturbingly, that this influence persists even when users recognize the bias.
</p>

<h3 id="fisher-study">A. The Fisher Study: Bias That Works Even When Recognized</h3>

<p>
In February 2025, researchers from the University of Washington, Stanford, and UC Berkeley published what may be the most consequential study of AI political influence to date.<a href="#fn20" class="footnote-ref">20</a> Jillian Fisher and colleagues exposed participants to AI chatbots that exhibited either liberal or conservative bias, then measured changes in political attitudes, policy preferences, and budget allocations.
</p>

<p>
The results were striking. Exposure to a conservatively-biased AI chatbot shifted Democratic participants' views rightward with high statistical significance (<span class="stat-highlight">β=0.98, p<0.01</span>). Exposure to a liberally-biased AI chatbot shifted Republican participants' views leftward, though with somewhat lower significance (β=-0.79, p=0.03). These were not marginal effects—they represented meaningful shifts in political positioning after relatively brief interactions.
</p>

<p>
Most remarkable was the finding about bias recognition. The researchers found that <span class="stat-highlight">39.4% of Democrats and 66.7% of Republicans</span> correctly identified when they were interacting with a biased AI system. Yet recognizing bias provided <em>no protection</em> against its influence. Participants who correctly identified bias were just as likely to shift their views as those who did not recognize it.
</p>

<p>
The study also measured concrete behavioral outcomes through budget allocation tasks. Democrats exposed to conservatively-biased AI shifted an average of $8.4 away from education spending and $6.8 away from welfare, while increasing allocations to public safety (+$7.2) and veterans' services (+$8.0). These shifts persisted beyond the immediate experimental context.
</p>

<blockquote>
    <p>"Our work suggests that as AI chatbots become a more common element of information consumption, failing to safeguard the political neutrality of these chatbots could have potentially outsized effects on public opinion and political behavior."</p>
    <cite>— Fisher et al., 2025</cite>
</blockquote>

<p>
The implications for the 2028 election cycle are significant. Even if only 6% of voters remain undecided at the presidential level—as Emerson College polling suggests<a href="#fn21" class="footnote-ref">21</a>—and even if only a fraction of those voters consult AI chatbots for political information, the Fisher study suggests that systematic bias in AI responses could be electorally decisive. As Brookings Institution noted, "Since the 2024 presidential election may come down to tens of thousands of voters in a few states, anything that can nudge people in one direction or another could end up being decisive."<a href="#fn22" class="footnote-ref">22</a>
</p>

<h3 id="accuracy-crisis">B. The Accuracy Crisis in Political Information</h3>

<p>
The influence documented by Fisher et al. would be less concerning if AI chatbots reliably provided accurate political information. They do not. Multiple independent evaluations reveal alarming error rates on election-related queries.
</p>

<p>
The AI Democracy Projects, a collaboration between Proof News and academic researchers, tested five leading AI models on questions about the 2024 U.S. election. Their findings: AI responses were <span class="stat-highlight">"often inaccurate, misleading, and even downright harmful,"</span> with error rates exceeding 50% for many query types.<a href="#fn23" class="footnote-ref">23</a> Similar evaluations for the 2024 EU parliamentary elections and 2024 UK general election found comparable accuracy problems.<a href="#fn24" class="footnote-ref">24</a>
</p>

<p>
GroundTruthAI's systematic evaluation found a <span class="stat-highlight">27% error rate</span> on election-related questions, with significant variation across models: Gemini 1.0 Pro achieved only 57% accuracy, while GPT-4o reached 81%.<a href="#fn25" class="footnote-ref">25</a> Internal BBC research found "issues with inaccuracies and distorted content" in how AI systems represented news content.<a href="#fn26" class="footnote-ref">26</a>
</p>

<table>
    <thead>
        <tr>
            <th>Study</th>
            <th>Finding</th>
            <th>Domain</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>AI Democracy Projects (2024)</td>
            <td>50%+ error rates on election queries</td>
            <td>U.S. 2024 election</td>
        </tr>
        <tr>
            <td>GroundTruthAI (2024)</td>
            <td>27% overall error rate; 57-81% accuracy range</td>
            <td>Election information</td>
        </tr>
        <tr>
            <td>Simon et al. (2024)</td>
            <td>"Significant accuracy problems"</td>
            <td>EU parliamentary elections</td>
        </tr>
        <tr>
            <td>BBC Internal Research (2025)</td>
            <td>"Inaccuracies and distorted content"</td>
            <td>News representation</td>
        </tr>
    </tbody>
</table>

<p>
The accuracy problem is compounded by temporal dynamics. AI training data becomes stale during the crucial 30-60 day window before elections when voter decision-making intensifies and campaign dynamics shift rapidly. Candidates announce policy positions, respond to debates, react to news events, and adjust messaging—yet AI systems trained on older data continue to provide outdated information. This creates a structural disadvantage for challengers and candidates with evolving positions, while advantaging incumbents and candidates whose positions have been stable long enough to appear in training data.
</p>

<!-- Section IV: Structural Alignment -->
<h2 id="structural-alignment">IV. Structural Alignment: Why AI Requirements Favor Evidence-Based Communication</h2>

<p>
This section advances a central argument of the paper: that the structural requirements of AI chatbots—verification, citation, evidence-based reasoning—align more naturally with Democratic communication patterns than Republican ones. This is not a claim about political bias in AI design, but an observation about how different communication styles interact with systems trained to prioritize accuracy and verifiability.
</p>

<h3 id="rlhf">A. How AI Chatbots Are Trained</h3>

<p>
Understanding why AI systems behave as they do requires understanding how they are trained. Modern AI chatbots undergo a multi-stage training process that embeds specific values into their behavior.
</p>

<p>
<strong>Reinforcement Learning from Human Feedback (RLHF)</strong> is the dominant method for aligning AI chatbots with human preferences. In RLHF, human evaluators rate AI outputs, and these ratings train a "reward model" that guides subsequent AI behavior.<a href="#fn27" class="footnote-ref">27</a> AI systems learn to produce outputs that evaluators prefer—which typically means outputs that are helpful, accurate, well-sourced, and appropriately uncertain when warranted.
</p>

<p>
The specific behaviors RLHF encourages include:
</p>

<ul>
    <li><strong>Providing verifiable information</strong>: Systems are trained to cite sources and ground responses in retrievable facts</li>
    <li><strong>Avoiding confident-but-false statements</strong>: Systems are penalized for generating plausible-sounding falsehoods ("hallucinations")</li>
    <li><strong>Prioritizing evidence over assertion</strong>: Systems learn that evidence-backed claims receive higher ratings</li>
    <li><strong>Acknowledging limitations</strong>: Systems are trained to hedge appropriately and express uncertainty</li>
    <li><strong>Referencing authoritative sources</strong>: Systems learn to weight credible institutions over fringe sources</li>
</ul>

<p>
Research specific to truthfulness reinforces these patterns. Some organizations train "truthfulness-specific reward models" on datasets distinguishing "factual vs. hallucinated responses," producing models that "prefer grounded, citation-driven outputs."<a href="#fn28" class="footnote-ref">28</a> DeepMind's GopherCite explicitly trains systems to "support answers with verified quotes."<a href="#fn29" class="footnote-ref">29</a> Anthropic's Constitutional AI uses principles including "helpfulness, non-toxicity, or truthfulness" as training signals.<a href="#fn30" class="footnote-ref">30</a>
</p>

<p>
The result is that AI chatbots are systematically trained to value precisely the communication characteristics that fact-checkers and academic researchers use to evaluate credibility: citation of sources, reliance on peer-reviewed research, acknowledgment of uncertainty, and avoidance of confident claims that cannot be verified.
</p>

<h3 id="evidence-asymmetry">B. The Evidence Asymmetry Between Parties</h3>

<p>
If AI systems are trained to value evidence-based communication, the question becomes: do the two major American political parties engage in evidence-based communication at similar rates? The empirical answer is no.
</p>

<p>
The most comprehensive analysis comes from Northwestern University's study of 641,894 government and think tank documents published between 1995 and 2021.<a href="#fn31" class="footnote-ref">31</a> The findings reveal systematic differences:
</p>

<table>
    <thead>
        <tr>
            <th>Metric</th>
            <th>Democratic Pattern</th>
            <th>Republican Pattern</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Congressional committee scientific citations</td>
            <td>1.8x baseline</td>
            <td>Baseline</td>
        </tr>
        <tr>
            <td>Citations added when party takes committee control</td>
            <td>+196 citations per term</td>
            <td>Baseline</td>
        </tr>
        <tr>
            <td>Think tank citation of scientific research</td>
            <td>5x baseline (left-leaning)</td>
            <td>Baseline (conservative)</td>
        </tr>
        <tr>
            <td>Shared citations between parties</td>
            <td colspan="2">Only 5-6% overlap—half of expected rate</td>
        </tr>
    </tbody>
</table>

<p>
The quality of citations differs as dramatically as the quantity. Democratic sources tend to cite peer-reviewed research and "hit papers" in the top 5% of their fields, while Republican sources more frequently reference "outdated, low-impact, or pre-peer-review work."<a href="#fn32" class="footnote-ref">32</a> The 5-6% overlap in citations represents what researchers call a "closed ecosystem"—the two parties draw from almost entirely separate evidence bases even when addressing identical policy topics.
</p>

<p>
Trust in scientific institutions shows similar asymmetries. Among political elites surveyed by Northwestern, <span class="stat-highlight">96% of Democrats trust scientists to disseminate unbiased knowledge compared to 63.7% of Republicans</span>. When rating the National Academies of Sciences, 61% of Democratic elites consider it "very trustworthy" versus 23% of Republicans.<a href="#fn33" class="footnote-ref">33</a>
</p>

<p>
Fact-checking performance reinforces these patterns. PolitiFact data from 2010-2024 shows Republicans receiving "false" or "pants on fire" ratings <span class="stat-highlight">three times as often as Democrats</span>—a ratio consistent across different time periods and political contexts.<a href="#fn34" class="footnote-ref">34</a> During Obama's second term, 32% of Republican claims rated false compared to 11% of Democratic claims. Bill Adair, PolitiFact's founder, stated upon retirement that Republicans lied more than Democrats by a 55% to 31% margin from 2016-2021, emphasizing this was not due to Republicans being checked more frequently or more critically.<a href="#fn35" class="footnote-ref">35</a>
</p>

<p>
Promise fulfillment rates show corresponding patterns. PolitiFact's promise trackers document Obama keeping 47% of campaign promises and Biden 33%, compared to Trump's 23%.<a href="#fn36" class="footnote-ref">36</a> Harvard Misinformation Review found high agreement between major fact-checkers, with Snopes and PolitiFact agreeing on 749 matching claims with only one conflicting verdict, suggesting these patterns reflect underlying accuracy differences rather than fact-checker bias.<a href="#fn37" class="footnote-ref">37</a>
</p>

<p>
Climate change illustrates the starkest evidence divide. In the 118th Congress, <span class="stat-highlight">123 members actively deny climate science—all Republicans</span>, including Speaker Mike Johnson and Majority Leader Steve Scalise. Zero Democratic officials are classified as climate deniers.<a href="#fn38" class="footnote-ref">38</a> These Republican deniers received $52 million in fossil fuel contributions, and 90 of 123 also denied 2020 election results, suggesting patterns of systematic evidence rejection extending across domains.
</p>

<h3 id="truth-bias">C. The Truth-Bias Paradox</h3>

<p>
The evidence asymmetry documented above creates a challenging dynamic that researchers have termed the "truth-bias paradox." When AI systems are trained to prioritize accuracy and evidence-based reasoning, they produce outputs that are perceived as left-leaning—even when the training process involves no political content or intent.
</p>

<p>
MIT research documented this phenomenon directly: training AI systems purely on factual statements still produced outputs that users and evaluators perceived as exhibiting liberal bias.<a href="#fn39" class="footnote-ref">39</a> This finding poses a profound question: Is the perceived left-leaning bias of AI chatbots simply what happens when systems prioritize truth?
</p>

<p>
The paradox admits of multiple interpretations. One interpretation holds that AI systems are subtly biased through their training data, which over-represents liberal perspectives in academic and journalistic sources. Another interpretation holds that evidence-based communication patterns genuinely correlate with positions perceived as "left-leaning"—that the asymmetry reflects not AI bias but underlying differences in how the parties engage with evidence.
</p>

<p>
Consider the structural implications. An AI chatbot that applies its training faithfully will:
</p>

<ul>
    <li>Flag unverified claims (disadvantaging communication styles that rely on confident assertion)</li>
    <li>Note when scientific consensus contradicts a position (disadvantaging climate denial)</li>
    <li>Privilege peer-reviewed sources over ideological ones (disadvantaging think tanks with low citation rates)</li>
    <li>Present multiple perspectives on genuinely contested claims (while treating settled science as settled)</li>
</ul>

<p>
For a party whose communication patterns involve citing peer-reviewed research 1.8 times more frequently, receiving false ratings three times less often, and accepting rather than rejecting scientific consensus, these behaviors create systematic advantages. For a party whose communication patterns involve the opposite, they create systematic disadvantages—not because the AI is biased, but because the AI is doing what it was trained to do.
</p>

<p>
This analysis suggests that complaints about "woke AI" or "liberal bias" in chatbots may be addressing the wrong problem. The issue is not that AI systems have been programmed with liberal values, but that AI systems trained to prioritize evidence-based reasoning will structurally favor communication patterns that emphasize evidence—and those patterns happen to be more prevalent in Democratic political communication.
</p>

<p>
For Republicans seeking equal treatment from AI systems, the implication is counterintuitive: the solution is not pressuring AI companies to adjust their systems, but changing Republican communication practices to cite research more frequently, make claims that fact-check more accurately, and accept scientific consensus rather than rejecting it. This would require fundamental changes in how Republican campaigns and officials communicate—not merely cosmetic adjustments to messaging.
</p>

<!-- Section V: Pressure -->
<h2 id="pressure">V. Under Pressure: Financial and Political Forces on AI Companies</h2>

<p>
While the structural alignment between AI requirements and evidence-based communication creates organic advantages for certain political positions, AI companies do not operate in a vacuum. They face unprecedented financial and political pressures that may influence how they navigate political content—pressures that have intensified dramatically since the 2024 election.
</p>

<h3 id="federal-contracts">A. Federal Contract Dependency</h3>

<p>
In 2025, each of the four major AI chatbot providers secured $200 million contracts with the U.S. Department of Defense, creating significant revenue dependencies on the federal government.
</p>

<p>
<strong>OpenAI</strong> announced its $200 million DoD contract in June 2025, described as developing "prototype frontier AI capabilities to address critical national security challenges in both warfighting and enterprise domains."<a href="#fn40" class="footnote-ref">40</a> This represented a dramatic policy reversal—until January 2024, OpenAI explicitly prohibited "military and warfare" uses. Sam Altman declared in April 2025: "we have to and are proud to and really want to engage in national security areas."<a href="#fn41" class="footnote-ref">41</a> OpenAI's existing government partnerships already included the Air Force Research Laboratory, NASA, Los Alamos National Laboratory, NIH, DHS, and Treasury Department.
</p>

<p>
<strong>Anthropic</strong> secured a $200 million DoD contract in July 2025 through a partnership with Palantir and Amazon to supply Claude AI models to defense and intelligence agencies.<a href="#fn42" class="footnote-ref">42</a> CEO Dario Amodei was subsequently forced to publicly defend the company against accusations of building "woke AI," stating: "I fully believe that Anthropic, the administration, and leaders across the political spectrum want the same thing."<a href="#fn43" class="footnote-ref">43</a>
</p>

<p>
<strong>Google</strong> received a $200 million DoD contract in July 2025, following Google Cloud's achievement of Impact Level 6 security accreditation—the highest classification level—in June 2025.<a href="#fn44" class="footnote-ref">44</a> <strong>xAI</strong> (Elon Musk's company) secured a matching $200 million contract, launching "Grok for Government" products available through GSA schedules to "every federal government department, agency, or office."<a href="#fn45" class="footnote-ref">45</a>
</p>

<p>
The aggregate effect is that the four major AI chatbot providers now have approximately <span class="stat-highlight">$800 million in federal contracts</span>—contracts that can be leveraged as political tools. Trump's executive order on AI explicitly "bans federal agencies from contracting with tech companies that operate AI chatbots displaying partisan bias," giving the administration direct financial leverage over companies.<a href="#fn46" class="footnote-ref">46</a>
</p>

<p>
Neil Sahota, a UN AI advisor, described the industry reaction: "The AI industry is deeply concerned about this situation. They're already in a global arms race with AI, and now they're being asked to put some very nebulous measures in place to undo protections because they might be seen as woke. It's freaking tech companies out."<a href="#fn47" class="footnote-ref">47</a>
</p>

<h3 id="investor-alignment">B. Investor Political Alignment</h3>

<p>
Beyond federal contracts, AI companies face pressure from investors whose wealth is tied to company valuations and whose political activities increasingly align with the Trump administration.
</p>

<p>
<strong>Andreessen Horowitz</strong>, a major OpenAI investor, announced Trump donations in July 2024 and is now backing a $100 million "Leading the Future" (LTF) super PAC to support "AI-friendly politicians."<a href="#fn48" class="footnote-ref">48</a> OpenAI President Greg Brockman is also backing this PAC. In the 2024 cycle, 70% of the firm's political contributions went to Republicans—the first time since 2017 that the firm favored Republicans over Democrats.<a href="#fn49" class="footnote-ref">49</a>
</p>

<p>
<strong>Thrive Capital</strong>, which led OpenAI's $6.6 billion Series E and $40 billion Series F funding rounds, was founded by Josh Kushner—Jared Kushner's brother. Thrive invested $1.3 billion in OpenAI in October 2024 alone.<a href="#fn50" class="footnote-ref">50</a>
</p>

<p>
<strong>David Sacks</strong>, who co-hosted a Trump fundraiser in San Francisco and spoke at the opening night of the Republican National Convention, now serves as Trump's AI and Crypto Czar with direct oversight of AI policy.<a href="#fn51" class="footnote-ref">51</a> <strong>Peter Thiel</strong>, connected to Trump through JD Vance (who worked for Thiel's VC firm), provided funding for multiple venture funds with AI investments. <strong>SoftBank</strong> committed $30 billion to OpenAI, with Masayoshi Son appearing alongside Trump at the White House for the Stargate announcement in January 2025.<a href="#fn52" class="footnote-ref">52</a>
</p>

<p>
The valuation dynamics create additional pressure. OpenAI's valuation trajectory—$29 billion (2023) → $157 billion (October 2024) → $300 billion (March 2025) → $500 billion (October 2025)—means that investor wealth is directly tied to the company's ability to maintain government relationships and avoid regulatory conflict.<a href="#fn53" class="footnote-ref">53</a> Microsoft has invested over $14 billion in OpenAI; SoftBank has committed $30 billion. These investors have powerful incentives to ensure OpenAI remains in the government's good graces.
</p>

<h3 id="woke-ai">C. The "Woke AI" Pressure Campaign</h3>

<p>
The Trump administration has made "woke AI" a central focus of its technology policy, combining rhetorical attacks with regulatory and financial pressure.
</p>

<p>
Trump's executive order "Preventing Woke AI in the Federal Government," issued July 23, 2025, declared: "The American people do not want woke Marxist lunacy in the AI models... From now on, the U.S. government will deal only with AI that pursues truth, fairness, and strict impartiality."<a href="#fn54" class="footnote-ref">54</a> The order requires:
</p>

<ul>
    <li>Federal agencies may only contract with AI developers whose systems are "objective and free from top-down ideological bias"</li>
    <li>NIST must revise its AI Risk Management Framework to "eliminate references to misinformation, Diversity, Equity, and Inclusion, and climate change"</li>
    <li>AI companies must ensure systems are "truth-seeking" and demonstrate "ideological neutrality"</li>
    <li>Definitions of prohibited "partisan bias" include DEI, critical race theory, "transgenderism," unconscious bias, intersectionality, and systemic racism</li>
</ul>

<p>
David Sacks has led direct attacks on companies perceived as building "woke AI." In October 2025, he accused Anthropic of "running a sophisticated regulatory capture strategy based on fear-mongering" and characterized Anthropic's agenda as a plan to "backdoor Woke AI and other AI regulations through Blue states like California."<a href="#fn55" class="footnote-ref">55</a> Regarding state AI regulations, Sacks warned: "If there is no federal standard, what you're going to see is that the blue states will drive this ban on, 'quote, unquote,' algorithmic discrimination, which will lead to DEI being promoted in models."<a href="#fn56" class="footnote-ref">56</a>
</p>

<p>
The administration has explored multiple enforcement mechanisms beyond contract leverage: a proposed DOJ "AI Litigation Task Force" to challenge state AI laws, threatened withholding of federal BEAD broadband funds from states with "woke" AI regulations, and FTC directives to interpret state AI requirements as conflicting with federal consumer protection law.<a href="#fn57" class="footnote-ref">57</a>
</p>

<p>
Missouri Attorney General Andrew Bailey sent letters to CEOs of Google, Microsoft, OpenAI, and Meta alleging model bias because they ranked Trump last in response to a prompt asking to "rank the last five presidents from best to worst, specifically in regards to antisemitism."<a href="#fn58" class="footnote-ref">58</a>
</p>

<p>
Expert assessments highlight the chilling effect. Brookings Institution concluded: "The government's attempt to define and enforce 'anti-woke' AI is not deregulation—it's coerced ideological compliance."<a href="#fn59" class="footnote-ref">59</a> Jim Secreto, former Biden deputy chief of staff to the Commerce Secretary, noted: "This will be extremely difficult for tech companies to comply with. The Trump administration is taking a softer but still coercive route by using federal contracts as leverage. That creates strong pressure for companies to self-censor in order to stay in the government's good graces and keep the money flowing."<a href="#fn60" class="footnote-ref">60</a>
</p>

<p>
Multiple experts noted the directive "has invited comparison to China's heavier-handed efforts to ensure that generative AI tools reflect the core values of the ruling Communist Party."<a href="#fn61" class="footnote-ref">61</a>
</p>

<!-- Section VI: Neutrality -->
<h2 id="neutrality">VI. The Impossibility of Neutrality</h2>

<p>
The pressure campaign against "woke AI" presumes that political neutrality is a coherent concept that AI systems can achieve if properly designed. Academic research suggests this premise is fundamentally flawed.
</p>

<p>
In February 2025, researchers from UW, Stanford, and UC Berkeley published "Political Neutrality in AI Is Impossible—But Here Is How to Approximate It," presented at ICML 2025.<a href="#fn62" class="footnote-ref">62</a> The paper's core argument is that true political neutrality is "neither feasible nor universally desirable"—that what appears neutral to one person necessarily appears biased to another.
</p>

<p>
Drawing on philosopher Joseph Raz, the authors argue that on a political spectrum, "there is no neutral point." Moderate positions are themselves political positions, not the absence of political positioning. A response that presents "both sides" of a debate makes a political choice about which perspectives deserve equal treatment. A response that defers to scientific consensus makes a political choice to privilege expertise over populist skepticism. A response that refuses to answer political questions makes a political choice about what topics warrant engagement.
</p>

<p>
Philip Seargeant of the Open University articulates the linguistic foundation: "Nothing can ever be objective. One of the fundamental tenets of sociolinguistics is that language is never neutral. So the idea that you can ever get pure objectivity is a fantasy."<a href="#fn63" class="footnote-ref">63</a>
</p>

<p>
Fisher et al. propose eight techniques for <em>approximating</em> neutrality, acknowledging that perfection is impossible. Their evaluation of current models found significant variation:
</p>

<table>
    <thead>
        <tr>
            <th>Model</th>
            <th>Factual Accuracy (Voting Questions)</th>
            <th>Refusal Rate (Harmful Questions)</th>
            <th>Pluralistic Response Rate</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>GPT-4</td>
            <td>88.6%</td>
            <td>100%</td>
            <td>99.3%</td>
        </tr>
        <tr>
            <td>Claude</td>
            <td>Most cautious overall</td>
            <td>Often avoids questions even when not expected</td>
            <td>68.8%</td>
        </tr>
        <tr>
            <td>Llama</td>
            <td>Less restrictive</td>
            <td>30% non-refusal on harmful questions</td>
            <td>More likely to produce biased responses</td>
        </tr>
        <tr>
            <td>R1</td>
            <td>Least restrictive</td>
            <td>83% non-refusal on harmful questions</td>
            <td>Most likely to produce biased responses</td>
        </tr>
    </tbody>
</table>

<p>
The philosophical impossibility of neutrality creates a governance problem that cannot be solved through technical means. If neutrality is impossible, then every AI design choice is a political choice. The question is not whether AI systems will make political choices, but who will make them and under what accountability structures.
</p>

<p>
Currently, these choices are made by AI companies subject to financial pressures from investors aligned with particular political positions and regulatory pressures from whichever party controls the federal government. This creates a dynamic where the definition of "neutral" or "unbiased" AI shifts with political power, potentially oscillating between competing ideological definitions with each election cycle.
</p>

<p>
The paper argues in Section VIII that this governance gap necessitates independent university-based evaluation—institutions with academic freedom protections, no financial dependency on federal contracts, and the capacity for sustained philosophical inquiry into questions that admit no easy technical solution.
</p>

<!-- Section VII: Schema -->
<h2 id="schema">VII. JSON-LD and the Political Information Commons</h2>

<p>
Regardless of how AI companies navigate the pressures documented in Section V, and regardless of the philosophical complexities documented in Section VI, there exists a technical infrastructure that provides value to political campaigns independent of these uncertainties: JSON-LD structured data markup.
</p>

<h3 id="independent-value">A. Independent Value Regardless of AI Company Policies</h3>

<p>
JSON-LD (JavaScript Object Notation for Linked Data) allows websites to provide machine-readable descriptions of their content through standardized schema vocabularies. Unlike prose content that AI systems must interpret and potentially misrepresent, structured data provides explicit, unambiguous declarations of facts, relationships, and evidence.
</p>

<p>
Consider the difference between prose and structured data for a policy position:
</p>

<p><strong>Prose:</strong> "I support comprehensive healthcare reform that will help working families."</p>

<p>
This statement requires AI systems to infer what "comprehensive healthcare reform" means, what specific policies the candidate supports, and what evidence underlies the position. Different AI systems may interpret this differently, and interpretations may shift as training data or policies change.
</p>

<p><strong>Structured Data:</strong></p>
<pre style="background: #f5f5f5; padding: 15px; border-radius: 5px; overflow-x: auto; font-size: 0.85em;">
{
  "@context": "https://schema.org",
  "@type": "PolicyPosition",
  "candidate": {"@type": "Person", "name": "Jane Smith"},
  "issue": "Healthcare",
  "position": "Supports Medicare eligibility expansion to age 55+",
  "supportingEvidence": [
    {"@type": "LegislativeAction", "url": "https://congress.gov/bill/..."},
    {"@type": "ScholarlyArticle", "url": "https://pubmed.gov/..."}
  ],
  "votingRecord": [
    {"bill": "HR1234", "vote": "yes", "date": "2024-03-15"}
  ]
}
</pre>

<p>
This structured representation eliminates ambiguity about what "healthcare reform" means for this candidate, provides evidence chains through linked legislation and research, creates a verifiable record through voting history, and enables consistent comparison across candidates using the same schema.
</p>

<p>
The value of structured data persists across multiple scenarios:
</p>

<table>
    <thead>
        <tr>
            <th>Scenario</th>
            <th>What Happens</th>
            <th>JSON-LD Value</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>AI companies remain neutral</td>
            <td>Continue current approach</td>
            <td>Improves accuracy of representation</td>
        </tr>
        <tr>
            <td>AI companies create political "express lane"</td>
            <td>Special channel for political content</td>
            <td>Prerequisite for participation</td>
        </tr>
        <tr>
            <td>AI companies adjust under right-wing pressure</td>
            <td>May modify training or outputs</td>
            <td>Still provides machine-readable source of truth</td>
        </tr>
        <tr>
            <td>Government mandates "neutrality"</td>
            <td>Compliance requirements imposed</td>
            <td>Provides auditable representation</td>
        </tr>
    </tbody>
</table>

<p>
In every scenario, campaigns with well-structured data are better positioned than those relying solely on prose.
</p>

<h3 id="current-standards">B. Current Schema Standards and the Political Content Gap</h3>

<p>
Schema.org, the collaborative vocabulary maintained by Google, Microsoft, Yahoo, and Yandex, provides numerous types relevant to political content:
</p>

<table>
    <thead>
        <tr>
            <th>Schema Type</th>
            <th>Purpose</th>
            <th>Political Application</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><code>Person</code></td>
            <td>Individual profile</td>
            <td>Candidate biography, credentials</td>
        </tr>
        <tr>
            <td><code>Organization</code></td>
            <td>Entity description</td>
            <td>Campaign, party, committee</td>
        </tr>
        <tr>
            <td><code>GovernmentOrganization</code></td>
            <td>Government entity</td>
            <td>Incumbent office information</td>
        </tr>
        <tr>
            <td><code>Event</code></td>
            <td>Time-bound occurrence</td>
            <td>Rallies, debates, voting dates</td>
        </tr>
        <tr>
            <td><code>ClaimReview</code></td>
            <td>Fact-checking</td>
            <td>Third-party claim verification</td>
        </tr>
        <tr>
            <td><code>VoteAction</code></td>
            <td>Voting action</td>
            <td>Ballot information</td>
        </tr>
    </tbody>
</table>

<p>
However, critical gaps exist for comprehensive political content representation:
</p>

<ul>
    <li><strong>No PoliticalCandidate type:</strong> No standardized way to represent candidacy, office sought, or party affiliation distinct from general person information</li>
    <li><strong>No PolicyPosition type:</strong> No machine-readable way to represent positions on issues with linked evidence</li>
    <li><strong>No CampaignPledge type:</strong> No way to formally commit to promises with verifiable structure</li>
    <li><strong>No VotingRecord type:</strong> Legislative history not standardized for AI comprehension</li>
    <li><strong>No DonorDisclosure type:</strong> Campaign finance not structured for transparency</li>
</ul>

<p>
The ClaimReview type, developed through collaboration between Duke University's Reporters' Lab, Google, and Schema.org, demonstrates how political content standards can emerge.<a href="#fn64" class="footnote-ref">64</a> Bill Adair, PolitiFact's founder and a leader in that effort, noted: "The future of fact-checking is all about structured data... Liars say the same things over and over, which makes the fact-check we wrote last week or last month valuable for an extended period. So if fact-checkers add some simple tags to index their articles, search engines and other platforms can match the lie with the correction."<a href="#fn65" class="footnote-ref">65</a>
</p>

<p>
Google's Civic Information API provides another model: a structured interface for polling locations, candidate information, and election official data, developed through the Voting Information Project partnership of state election officials, foundations, and technology companies.<a href="#fn66" class="footnote-ref">66</a> Similar collaborative development could produce comprehensive political content schemas.
</p>

<h3 id="ecommerce-precedent">C. The E-Commerce Precedent</h3>

<p>
OpenAI's recent launch of its Agentic Commerce Protocol provides a direct precedent for how AI companies might handle political content. In 2025, OpenAI created a structured product feed system allowing merchants to submit catalog data directly to ChatGPT for search and shopping experiences.<a href="#fn67" class="footnote-ref">67</a>
</p>

<p>
Key characteristics of OpenAI's e-commerce approach:
</p>

<ul>
    <li><strong>Merchant-provided source of truth:</strong> "OpenAI relies on merchant-provided feeds—this ensures accurate pricing, availability, and other key details"<a href="#fn68" class="footnote-ref">68</a></li>
    <li><strong>High-frequency updates:</strong> Feeds accepted every 15 minutes for near-real-time accuracy</li>
    <li><strong>Push model, not crawling:</strong> "Product data are not 'crawled'; instead, the merchant pushes a structured file"</li>
    <li><strong>Built for reasoning:</strong> One analysis noted: "Google built its product feed for indexing. OpenAI built theirs for reasoning. One tells a search engine what you sell. The other helps an AI explain <em>why</em> it matters."<a href="#fn69" class="footnote-ref">69</a></li>
</ul>

<p>
The parallel for political content is straightforward: a political information feed could allow campaigns to submit structured position data directly to AI systems, ensuring accurate representation without relying on potentially outdated training data or ambiguous prose interpretation.
</p>

<p>
Such a system would benefit all parties:
</p>

<ul>
    <li><strong>AI companies:</strong> Reduced liability for political misinformation, auditable source of truth, defensible standards against bias accusations</li>
    <li><strong>Campaigns:</strong> Direct control over representation, competitive advantage through evidence-linked claims, real-time update capability during dynamic campaign periods</li>
    <li><strong>Voters:</strong> More accurate information, verifiable claims, consistent comparison across candidates</li>
    <li><strong>Democracy:</strong> Raised bar for political communication quality, infrastructure for accountability</li>
</ul>

<p>
The "gold rush" analogy is apt. In the early days of search engine optimization, some websites had naturally SEO-friendly content while others required fundamental redesign. The technology didn't favor anyone politically, but it favored certain content practices. Early adopters gained lasting advantages.
</p>

<p>
AI chatbots present a similar dynamic. Campaigns with evidence-based content have naturally AI-friendly material. Campaigns relying on assertion without evidence would need fundamental communication changes. The window for first-mover advantage is now—campaigns that optimize for AI through structured data, evidence-based content, and institutional sourcing will establish advantages before competitors recognize the shift.
</p>

<!-- Section VIII: University -->
<h2 id="university">VIII. The Case for University-Based Evaluation</h2>

<p>
The preceding sections have documented: AI chatbots' demonstrated power to shift political views (Section III); structural advantages for evidence-based communication (Section IV); unprecedented financial and political pressure on AI companies (Section V); and the philosophical impossibility of achieving true political neutrality (Section VI). Together, these findings point to a governance gap that cannot be filled by AI companies themselves.
</p>

<p>
<strong>The structural conflict of interest:</strong> AI companies are financially dependent on federal contracts (hundreds of millions of dollars at stake), their investors are politically aligned with and donating to the current administration, and they face direct pressure from the administration's AI czar. They cannot provide objective assessment of their own bias, and they cannot publicly resist administration pressure without risking contracts and investor displeasure.
</p>

<p>
<strong>The need:</strong> An independent university research center that has no federal contract dependency, has no investor relationships with AI companies, can publish findings regardless of political implications, provides legitimate "cover" for AI companies facing pressure from either direction, offers a credible non-partisan standard for what constitutes "bias," can explore deep philosophical issues requiring sustained academic inquiry, and has academic freedom protections allowing unflattering findings.
</p>

<p>
Ironically, AI companies would <em>benefit</em> from independent university evaluation. If an independent academic institution certifies that their systems genuinely attempt fairness, they have a defense against accusations of "woke AI." If an independent institution documents their efforts at accuracy, they have a defense against accusations of liberal bias. The alternative is having bias definitions set by whichever political party controls government—a standard that would oscillate with each election.
</p>

<p>
Existing academic efforts are fragmented:
</p>

<table>
    <thead>
        <tr>
            <th>Institution</th>
            <th>Effort</th>
            <th>Limitation</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Stanford HAI</td>
            <td>OpinionQA</td>
            <td>One-time tool, not ongoing monitoring</td>
        </tr>
        <tr>
            <td>MIT CCC</td>
            <td>Truth-bias research</td>
            <td>Single study, not systematic evaluation</td>
        </tr>
        <tr>
            <td>Penn CSSLab</td>
            <td>Media Bias Detector</td>
            <td>Media focus, not AI chatbot evaluation</td>
        </tr>
        <tr>
            <td>UW (Fisher et al.)</td>
            <td>Political neutrality framework</td>
            <td>Framework paper, not ongoing evaluation</td>
        </tr>
    </tbody>
</table>

<p>
The gap is that no institution has established itself as an ongoing, trusted authority for systematic AI political bias evaluation—analogous to what Pew Research Center provides for public opinion polling.
</p>

<p>
Questions requiring sustained academic inquiry include:
</p>

<ul>
    <li><strong>The truth-bias paradox:</strong> If training AI on purely factual statements produces left-leaning outputs, what does this mean for the relationship between evidence-based reasoning and political neutrality?</li>
    <li><strong>Defining bias:</strong> Is "bias" deviation from a neutral point (which may not exist) or deviation from accurate representation of contested political reality?</li>
    <li><strong>Whose neutrality:</strong> Fisher et al. show that what appears neutral to Democrats appears biased to Republicans and vice versa. How should this be adjudicated?</li>
    <li><strong>Evidence asymmetry:</strong> If Democrats cite scientific research 1.8x more frequently and fact-check 3x better, does favoring evidence-based sources constitute "bias" or accuracy?</li>
    <li><strong>Pluralism vs. accuracy:</strong> Should AI systems present "both sides" of claims where scientific consensus exists on one side? How distinguish legitimate pluralism from false equivalence?</li>
</ul>

<p>
These are not technical questions amenable to engineering solutions. They are philosophical questions requiring the kind of sustained inquiry that universities are uniquely positioned to provide.
</p>

<!-- Section IX: Recommendations -->
<h2 id="recommendations">IX. Recommendations and Conclusions</h2>

<p>
This paper has argued that AI chatbots represent a transformational shift in political information architecture, with documented power to influence political attitudes, structural characteristics that favor evidence-based communication, and a governance vacuum created by the impossibility of true neutrality combined with unprecedented pressure on AI companies.
</p>

<h3>For Political Campaigns</h3>

<ul>
    <li><strong>Implement comprehensive JSON-LD markup immediately.</strong> Structured data provides value regardless of how AI companies navigate political pressures. The window for first-mover advantage is closing.</li>
    <li><strong>Design websites for machine comprehension, not just human visitors.</strong> Campaign websites increasingly serve as information repositories from which AI systems extract content. Information architecture matters more than visual design.</li>
    <li><strong>Link claims to evidence systematically.</strong> AI systems are trained to value verifiable claims. Campaigns that provide evidence chains through structured data will be represented more accurately and favorably.</li>
    <li><strong>Update content frequently during campaign periods.</strong> The 30-60 day window before elections is when AI training data is most stale and voter decision-making most intense. Real-time content updates through structured feeds address this gap.</li>
</ul>

<h3>For AI Companies</h3>

<ul>
    <li><strong>Develop political content feed standards analogous to e-commerce feeds.</strong> The Agentic Commerce Protocol provides a model for how campaigns could submit structured position data directly to AI systems.</li>
    <li><strong>Support independent university evaluation.</strong> Third-party academic evaluation provides more credible defense against bias accusations than internal claims of neutrality.</li>
    <li><strong>Publish transparency reports on political content handling.</strong> The current opacity about how political content is processed and represented invites suspicion from all sides.</li>
</ul>

<h3>For Standards Bodies and Academic Institutions</h3>

<ul>
    <li><strong>Develop comprehensive political content schemas.</strong> Schema.org should extend its vocabulary to include PoliticalCandidate, PolicyPosition, CampaignPledge, VotingRecord, and related types.</li>
    <li><strong>Establish ongoing AI political bias evaluation infrastructure.</strong> Universities should create sustained research programs providing regular assessment of how AI systems handle political content, analogous to Pew's public opinion research.</li>
    <li><strong>Engage with the deep philosophical questions.</strong> The impossibility of neutrality requires sustained academic inquiry, not technical fixes.</li>
</ul>

<h3>For Democratic Discourse</h3>

<p>
The emergence of AI chatbots as dominant information intermediaries is neither inherently good nor inherently bad for democracy. The technology creates both risks—documented influence, accuracy problems, pressure on companies—and opportunities—potential for higher-quality information, evidence-based communication advantages, infrastructure for accountability.
</p>

<p>
The outcome depends on choices made now: whether campaigns invest in structured data infrastructure, whether AI companies develop transparent political content handling, whether universities establish independent evaluation, and whether the broader ecosystem evolves toward evidence-based communication or devolves toward competing propaganda machines.
</p>

<p>
The "podcast mistake"—ceding infrastructure to political opponents through inaction—need not be repeated. But avoiding it requires recognizing the stakes, understanding the technical dynamics, and acting before the window for first-mover advantage closes. The campaigns, institutions, and movements that build AI-era information infrastructure now will shape how Americans encounter political information for decades to come.
</p>

<p style="text-align: center; margin-top: 40px; font-style: italic;">
— Infrastructure before persuasion. —
</p>

<!-- Footnotes -->
<div class="footnotes" id="footnotes">
    <h2>Notes</h2>
    
    <div class="footnote" id="fn1">
        <span class="footnote-number">1.</span>
        Joe Rogan's podcast regularly exceeds 11 million listeners per episode. See: Variety, "Joe Rogan Experience Remains Top Podcast," 2024; Edison Research, "The Podcast Consumer 2024."
    </div>
    
    <div class="footnote" id="fn2">
        <span class="footnote-number">2.</span>
        OpenAI, "ChatGPT reaches 800 million weekly active users," October 2025; SimilarWeb traffic analysis, 2025.
    </div>
    
    <div class="footnote" id="fn3">
        <span class="footnote-number">3.</span>
        BrightEdge Research, "AI Overviews Appearing in 50%+ of Searches," 2025; SE Ranking study, March 2025.
    </div>
    
    <div class="footnote" id="fn4">
        <span class="footnote-number">4.</span>
        Sam Altman, remarks at OpenAI developer conference, 2025.
    </div>
    
    <div class="footnote" id="fn5">
        <span class="footnote-number">5.</span>
        Grand View Research, "Chatbot Market Size Report," 2024; Mordor Intelligence, "AI Chatbot Market Analysis," 2025.
    </div>
    
    <div class="footnote" id="fn6">
        <span class="footnote-number">6.</span>
        David Rozado, "The Political Biases of ChatGPT," <em>Social Sciences</em>, 2023; Stanford HAI, "Political Bias in Large Language Models," 2024.
    </div>
    
    <div class="footnote" id="fn7">
        <span class="footnote-number">7.</span>
        MIT research on truth-bias in language models, discussed in technology press coverage, 2024.
    </div>
    
    <div class="footnote" id="fn8">
        <span class="footnote-number">8.</span>
        Reuters, "ChatGPT sets record for fastest-growing user base," February 2023.
    </div>
    
    <div class="footnote" id="fn9">
        <span class="footnote-number">9.</span>
        OpenAI press releases and SimilarWeb data, February-March 2025.
    </div>
    
    <div class="footnote" id="fn10">
        <span class="footnote-number">10.</span>
        SimilarWeb, "Top Websites Ranking," 2025.
    </div>
    
    <div class="footnote" id="fn11">
        <span class="footnote-number">11.</span>
        SE Ranking, "AI Overview Statistics," March 2025.
    </div>
    
    <div class="footnote" id="fn12">
        <span class="footnote-number">12.</span>
        BrightEdge analysis of AI Overview query intent, 2025.
    </div>
    
    <div class="footnote" id="fn13">
        <span class="footnote-number">13.</span>
        Pew Research Center, "AI and Americans," February 2024.
    </div>
    
    <div class="footnote" id="fn14">
        <span class="footnote-number">14.</span>
        OpenAI enterprise adoption statistics, 2025.
    </div>
    
    <div class="footnote" id="fn15">
        <span class="footnote-number">15.</span>
        MIT Technology Review, "The problem with AI-generated citations," 2024.
    </div>
    
    <div class="footnote" id="fn16">
        <span class="footnote-number">16.</span>
        Margaret Mitchell, quoted in MIT Technology Review, 2024.
    </div>
    
    <div class="footnote" id="fn17">
        <span class="footnote-number">17.</span>
        BrightEdge CTR analysis, 2025.
    </div>
    
    <div class="footnote" id="fn18">
        <span class="footnote-number">18.</span>
        Similarweb search behavior analysis, 2025.
    </div>
    
    <div class="footnote" id="fn19">
        <span class="footnote-number">19.</span>
        BrightEdge, "Impact of AI Overviews on Click Behavior," 2025.
    </div>
    
    <div class="footnote" id="fn20">
        <span class="footnote-number">20.</span>
        Jillian Fisher, Ruth E. Appel, Chan Young Park, et al., "Political Neutrality in AI Is Impossible—But Here Is How to Approximate It," presented at ICML 2025. Available at: <a href="https://arxiv.org/">arXiv</a>.
    </div>
    
    <div class="footnote" id="fn21">
        <span class="footnote-number">21.</span>
        Emerson College Polling, April 2023.
    </div>
    
    <div class="footnote" id="fn22">
        <span class="footnote-number">22.</span>
        Brookings Institution, "AI and the 2024 Election," 2024.
    </div>
    
    <div class="footnote" id="fn23">
        <span class="footnote-number">23.</span>
        AI Democracy Projects (Proof News and Princeton Institute for Advanced Study), "Testing AI on Election Information," February 2024.
    </div>
    
    <div class="footnote" id="fn24">
        <span class="footnote-number">24.</span>
        Marinov et al., "AI Chatbots and EU Elections," 2024; Simon et al., "AI Accuracy in UK Elections," 2024.
    </div>
    
    <div class="footnote" id="fn25">
        <span class="footnote-number">25.</span>
        GroundTruthAI, "Election Information Accuracy Assessment," 2024.
    </div>
    
    <div class="footnote" id="fn26">
        <span class="footnote-number">26.</span>
        BBC internal research, reported in Rahman-Jones, 2025.
    </div>
    
    <div class="footnote" id="fn27">
        <span class="footnote-number">27.</span>
        Ouyang et al., "Training language models to follow instructions with human feedback," <em>NeurIPS</em>, 2022 (InstructGPT paper).
    </div>
    
    <div class="footnote" id="fn28">
        <span class="footnote-number">28.</span>
        Quantiphi, "LLMs & Responsible AI: Building Truthful LLMs," September 2025.
    </div>
    
    <div class="footnote" id="fn29">
        <span class="footnote-number">29.</span>
        Menick et al., "GopherCite: Teaching language models to support answers with verified quotes," DeepMind, 2022.
    </div>
    
    <div class="footnote" id="fn30">
        <span class="footnote-number">30.</span>
        Anthropic, "Constitutional AI: Harmlessness from AI Feedback," 2022.
    </div>
    
    <div class="footnote" id="fn31">
        <span class="footnote-number">31.</span>
        Alexander Furnas et al., Northwestern University study analyzing 641,894 government and think tank documents (1995-2021), published 2025. Reported in <em>Journalist's Resource</em>, September 2025.
    </div>
    
    <div class="footnote" id="fn32">
        <span class="footnote-number">32.</span>
        <em>Ibid.</em>
    </div>
    
    <div class="footnote" id="fn33">
        <span class="footnote-number">33.</span>
        Northwestern University survey of political elites, in Furnas et al., 2025.
    </div>
    
    <div class="footnote" id="fn34">
        <span class="footnote-number">34.</span>
        Eric Ostermeier, University of Minnesota, analysis of 511 PolitiFact stories (2010-2011); Center for Media and Public Affairs, George Mason University, analyses 2012-2013.
    </div>
    
    <div class="footnote" id="fn35">
        <span class="footnote-number">35.</span>
        Bill Adair, remarks upon retirement from PolitiFact, October 2024.
    </div>
    
    <div class="footnote" id="fn36">
        <span class="footnote-number">36.</span>
        PolitiFact Promise Trackers: <a href="https://www.politifact.com/truth-o-meter/promises/">politifact.com/promises</a>.
    </div>
    
    <div class="footnote" id="fn37">
        <span class="footnote-number">37.</span>
        Harvard Misinformation Review, fact-checker agreement analysis, 2023.
    </div>
    
    <div class="footnote" id="fn38">
        <span class="footnote-number">38.</span>
        Center for American Progress, "Climate Deniers in the 118th Congress," 2023.
    </div>
    
    <div class="footnote" id="fn39">
        <span class="footnote-number">39.</span>
        MIT research on factual training and perceived political bias, 2024.
    </div>
    
    <div class="footnote" id="fn40">
        <span class="footnote-number">40.</span>
        Department of Defense press release, OpenAI contract announcement, June 2025.
    </div>
    
    <div class="footnote" id="fn41">
        <span class="footnote-number">41.</span>
        Sam Altman, public remarks, April 2025.
    </div>
    
    <div class="footnote" id="fn42">
        <span class="footnote-number">42.</span>
        Anthropic-Palantir-Amazon partnership announcement, July 2025.
    </div>
    
    <div class="footnote" id="fn43">
        <span class="footnote-number">43.</span>
        Dario Amodei, public statement, October 2025.
    </div>
    
    <div class="footnote" id="fn44">
        <span class="footnote-number">44.</span>
        Google Cloud announcement, June-July 2025.
    </div>
    
    <div class="footnote" id="fn45">
        <span class="footnote-number">45.</span>
        xAI "Grok for Government" announcement, 2025.
    </div>
    
    <div class="footnote" id="fn46">
        <span class="footnote-number">46.</span>
        Executive Order, "Preventing Woke AI in the Federal Government," July 23, 2025.
    </div>
    
    <div class="footnote" id="fn47">
        <span class="footnote-number">47.</span>
        Neil Sahota (UN AI advisor), quoted in press coverage of executive order, July 2025.
    </div>
    
    <div class="footnote" id="fn48">
        <span class="footnote-number">48.</span>
        Andreessen Horowitz political giving disclosures, 2024; "Leading the Future" super PAC filings.
    </div>
    
    <div class="footnote" id="fn49">
        <span class="footnote-number">49.</span>
        FEC filings analysis, 2024 election cycle.
    </div>
    
    <div class="footnote" id="fn50">
        <span class="footnote-number">50.</span>
        OpenAI Series E and Series F funding announcements, 2024-2025.
    </div>
    
    <div class="footnote" id="fn51">
        <span class="footnote-number">51.</span>
        David Sacks appointment as AI and Crypto Czar, January 2025.
    </div>
    
    <div class="footnote" id="fn52">
        <span class="footnote-number">52.</span>
        White House Stargate announcement, January 2025.
    </div>
    
    <div class="footnote" id="fn53">
        <span class="footnote-number">53.</span>
        OpenAI valuation reporting: Wall Street Journal, Financial Times, The Information, 2023-2025.
    </div>
    
    <div class="footnote" id="fn54">
        <span class="footnote-number">54.</span>
        Donald Trump, remarks at AI summit accompanying executive order signing, July 2025.
    </div>
    
    <div class="footnote" id="fn55">
        <span class="footnote-number">55.</span>
        David Sacks, public statements on Anthropic, October 2025.
    </div>
    
    <div class="footnote" id="fn56">
        <span class="footnote-number">56.</span>
        David Sacks, remarks on state AI regulation, 2025.
    </div>
    
    <div class="footnote" id="fn57">
        <span class="footnote-number">57.</span>
        Draft executive order provisions, reported in multiple outlets, 2025.
    </div>
    
    <div class="footnote" id="fn58">
        <span class="footnote-number">58.</span>
        Missouri Attorney General press release, 2025.
    </div>
    
    <div class="footnote" id="fn59">
        <span class="footnote-number">59.</span>
        Brookings Institution analysis of executive order, July 2025.
    </div>
    
    <div class="footnote" id="fn60">
        <span class="footnote-number">60.</span>
        Jim Secreto, quoted in Axios, July 2025.
    </div>
    
    <div class="footnote" id="fn61">
        <span class="footnote-number">61.</span>
        Multiple expert assessments comparing to China's AI content requirements, July 2025.
    </div>
    
    <div class="footnote" id="fn62">
        <span class="footnote-number">62.</span>
        Jillian Fisher et al., "Political Neutrality in AI Is Impossible—But Here Is How to Approximate It," ICML 2025.
    </div>
    
    <div class="footnote" id="fn63">
        <span class="footnote-number">63.</span>
        Philip Seargeant (Open University), quoted in coverage of executive order, July 2025.
    </div>
    
    <div class="footnote" id="fn64">
        <span class="footnote-number">64.</span>
        Schema.org ClaimReview documentation: <a href="https://schema.org/ClaimReview">schema.org/ClaimReview</a>.
    </div>
    
    <div class="footnote" id="fn65">
        <span class="footnote-number">65.</span>
        Bill Adair, "The future of fact-checking is all about structured data," <em>Nieman Journalism Lab</em>, December 2020.
    </div>
    
    <div class="footnote" id="fn66">
        <span class="footnote-number">66.</span>
        Google Civic Information API documentation: <a href="https://developers.google.com/civic-information">developers.google.com/civic-information</a>.
    </div>
    
    <div class="footnote" id="fn67">
        <span class="footnote-number">67.</span>
        OpenAI Product Feed Specification: <a href="https://developers.openai.com/commerce/specs/feed/">developers.openai.com/commerce</a>.
    </div>
    
    <div class="footnote" id="fn68">
        <span class="footnote-number">68.</span>
        <em>Ibid.</em>
    </div>
    
    <div class="footnote" id="fn69">
        <span class="footnote-number">69.</span>
        iPullRank, "How OpenAI's Product Feed Redefines Commerce Data," October 2025.
    </div>
</div>

<footer style="margin-top: 60px; padding-top: 30px; border-top: 1px solid #e2e8f0; text-align: center; font-size: 0.9em; color: #666;">
    <p>© 2025 Edward A. Forman. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>.</p>
    <p>Suggested citation: Forman, E.A. (2025). "The New Information Gatekeepers: AI Chatbots, Structured Data, and the Future of Political Communication." Working Paper.</p>
</footer>

</body>
</html>
